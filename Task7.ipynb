{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a79a076f",
   "metadata": {},
   "source": [
    "1. What is the difference between Bagging and Boosting?\n",
    "\n",
    "Bagging (Bootstrap Aggregating) builds multiple independent base models (e.g., decision trees) on different bootstrap samples of the training data and averages (for regression) or votes (for classification) their predictions to reduce variance.\n",
    "\n",
    "\n",
    "Boosting sequentially trains models where each new model focuses on correcting the errors of the previous ones. Predictions are combined through a weighted sum, reducing bias and potentially variance but risking overfitting.\n",
    "\n",
    "2. How does Random Forest reduce variance?\n",
    "\n",
    "Random Forest extends bagging by introducing feature randomness: each split in each tree is chosen from a random subset of features rather than all features. This de-correlates the trees, so that averaging their predictions more effectively reduces overall variance.\n",
    "\n",
    "3. What is the weakness of boosting-based methods?\n",
    "\n",
    "Boosting can overfit noisy data since it aggressively focuses on misclassified examples, amplifying noise and outliers. It is also more sensitive to learning rate and requires careful tuning of hyperparameters (e.g., number of estimators, learning rate), which can increase training time and complexity.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
