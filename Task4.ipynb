{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d480b7",
   "metadata": {},
   "source": [
    "1. What is entropy and information gain?\n",
    "\n",
    "Entropy measures the uncertainty or impurity in a dataset. Given a set $S$ with class proportions $p_i$, entropy is defined as:\n",
    "    H(S) = -sum_i p_ilog_2(p_i).\n",
    "Information Gain quantifies the reduction in entropy achieved by splitting on a particular feature. For feature $A$,\n",
    "\n",
    "    IG(S, A) = H(S) - sum_{v in {values}(A)} frac{|S_v|}{|S|} *H(S_v).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3990acd",
   "metadata": {},
   "source": [
    "2. Explain the difference between Gini Index and Entropy.\n",
    "\n",
    "Gini Index measures impurity as G(S) = 1 - sum_ip_i^2. It represents the probability of misclassifying a randomly chosen element if labeled according to the distribution.\n",
    "Entropy uses the plog_2p formulation from information theory. While both assess impurity, Gini is computationally simpler (no logarithms) and may lead to similar splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82459821",
   "metadata": {},
   "source": [
    "3. How can a decision tree overfit? How can this be avoided?\n",
    "\n",
    "Overfitting occurs when a tree grows too deep, learning noise or outliers in the training data and failing to generalize.\n",
    "Avoidance strategies include:\n",
    "\n",
    "  1. Pruning (post- or pre-pruning) to remove branches that add little information.\n",
    "  2. Limiting tree depth with max_depth.\n",
    "  3. Setting a minimum number of samples** per leaf or split (min_samples_leaf, min_samples_split)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
